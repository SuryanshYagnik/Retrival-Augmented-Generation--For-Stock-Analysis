{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Retrieval Augmented Generation (RAG)\n\nLarge language models (LLMs) like ChatGPT are great at understanding language and generating fluent text. However, sometimes they struggle with factual accuracy or keeping information up to date. Retrieval augmented generation (RAG) solves this by adding a \"research assistant\" step:\n\n1. **Retrieval**: When you give the LLM a prompt or question, RAG first searches through a database of texts üìú - like having access to a giant virtual library! It retrieves relevant snippets of information that could be useful for composing its response.\n\n2. **Augmentation**: Those retrieved context passages are then incorporated into the prompt to the LLM üìù, giving it an information source to base the answer on. Just like reading research notes from a database and integrating them into your understanding before writing on a topic.\n\n3. **Generation**: Finally, the LLM leverages the augmented context to expand its knowledge and language capabilities to generate a response. Making the text produced not just fluent, but also accurate and factual, since it's based on relevant reference material.\n\nIn essence, RAG reduces the LLM's chance of hallucinating because now it gets to consult a knowledge base before responding. This makes responses more reliable and trustworthy, especially for topics requiring specific up-to-date facts.","metadata":{}},{"cell_type":"markdown","source":"<img src=\"https://python.langchain.com/assets/images/vector_stores-125d1675d58cfb46ce9054c9019fea72.jpg\" height=800 width=800/>\n\n‚≠ê Photo credits: [Langchain](https://python.langchain.com/docs/modules/data_connection/vectorstores/)","metadata":{}},{"cell_type":"markdown","source":"### **Retrieval**\n\nTo use RAG, we need to have a database of documents that can provide relevant information for our queries. In this tutorial, we will create a database from the book \"How to Build a Career in AI\" by Andrew NG. We will use Langchain, Chroma, and Hugging Face to perform RAG on this book.\n\nThe process of creating a database involves the following steps:\n\n- **Chunking**: We divide the book into smaller pieces, such as paragraphs or sentences, that can be easily indexed and retrieved.\n\n- **Embedding**: We use a pre-trained model from Hugging Face to convert each chunk into a vector representation, also known as a sentence embedding. This captures the semantic meaning of the chunk and allows us to compare it with other chunks or queries.\n\nüí°: For more information on vector embeddings check out the word embeddings section in my last lesson at [Notebook Link](https://www.kaggle.com/code/nathanyoung1/transformer-based-language-translation-in-pytorch). The word embeddings are combined to form sentence embeddings which we will refer to as vector embeddings throughout this tutorial.\n\n- **Indexing**: We store the vector embeddings in a vector database, such as Chroma, that can efficiently perform similarity search. This means that given a query vector, we can find the most similar vectors in the database, and retrieve the corresponding chunks.\n\nWhen we want to use RAG to generate a response for a query, we first embed the query using the same model as before. Then, as shown in the image above üëÜ we use the vector database to find the most similar embeddings to the query embedding. These similar embeddings are linked to particular chunks of our document. We then fed this chunks as context to the LLM, enabling it to generate a coherent and informative answer.","metadata":{}},{"cell_type":"code","source":"# install the vector database, langchain, pypdf, hugging face sentence_transformers\n!pip install chromadb langchain pypdf sentence_transformers","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:25:33.537272Z","iopub.execute_input":"2024-02-24T14:25:33.537530Z","iopub.status.idle":"2024-02-24T14:26:14.878463Z","shell.execute_reply.started":"2024-02-24T14:25:33.537508Z","shell.execute_reply":"2024-02-24T14:26:14.877490Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting chromadb\n  Downloading chromadb-0.4.23-py3-none-any.whl.metadata (7.3 kB)\nCollecting langchain\n  Downloading langchain-0.1.9-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: pypdf in /opt/conda/lib/python3.10/site-packages (4.0.0)\nCollecting sentence_transformers\n  Downloading sentence_transformers-2.4.0-py3-none-any.whl.metadata (11 kB)\nCollecting build>=1.0.3 (from chromadb)\n  Downloading build-1.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: requests>=2.28 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.31.0)\nRequirement already satisfied: pydantic>=1.9 in /opt/conda/lib/python3.10/site-packages (from chromadb) (2.5.3)\nCollecting chroma-hnswlib==0.7.3 (from chromadb)\n  Downloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\nRequirement already satisfied: fastapi>=0.95.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.108.0)\nRequirement already satisfied: uvicorn>=0.18.3 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.25.0)\nRequirement already satisfied: numpy>=1.22.5 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.24.4)\nCollecting posthog>=2.4.0 (from chromadb)\n  Downloading posthog-3.4.2-py2.py3-none-any.whl.metadata (2.0 kB)\nRequirement already satisfied: typing-extensions>=4.5.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.9.0)\nCollecting pulsar-client>=3.1.0 (from chromadb)\n  Downloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.0 kB)\nCollecting onnxruntime>=1.14.1 (from chromadb)\n  Downloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.2 kB)\nRequirement already satisfied: opentelemetry-api>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.44b0-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (1.22.0)\nRequirement already satisfied: tokenizers>=0.13.2 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.15.1)\nCollecting pypika>=0.48.9 (from chromadb)\n  Downloading PyPika-0.48.9.tar.gz (67 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: tqdm>=4.65.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.66.1)\nRequirement already satisfied: overrides>=7.3.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (7.4.0)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.1.1)\nCollecting grpcio>=1.58.0 (from chromadb)\n  Downloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nCollecting bcrypt>=4.0.1 (from chromadb)\n  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (9.5 kB)\nRequirement already satisfied: typer>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (0.9.0)\nCollecting kubernetes>=28.1.0 (from chromadb)\n  Downloading kubernetes-29.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\nRequirement already satisfied: tenacity>=8.2.3 in /opt/conda/lib/python3.10/site-packages (from chromadb) (8.2.3)\nRequirement already satisfied: PyYAML>=6.0.0 in /opt/conda/lib/python3.10/site-packages (from chromadb) (6.0.1)\nRequirement already satisfied: mmh3>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from chromadb) (4.1.0)\nCollecting orjson>=3.9.12 (from chromadb)\n  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (49 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain) (2.0.25)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/conda/lib/python3.10/site-packages (from langchain) (3.9.1)\nRequirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain) (4.0.3)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain) (0.6.3)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain) (1.33)\nCollecting langchain-community<0.1,>=0.0.21 (from langchain)\n  Downloading langchain_community-0.0.24-py3-none-any.whl.metadata (8.1 kB)\nCollecting langchain-core<0.2,>=0.1.26 (from langchain)\n  Downloading langchain_core-0.1.26-py3-none-any.whl.metadata (6.0 kB)\nCollecting langsmith<0.2.0,>=0.1.0 (from langchain)\n  Downloading langsmith-0.1.7-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: transformers<5.0.0,>=4.32.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (4.37.0)\nRequirement already satisfied: torch>=1.11.0 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.15.1 in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (0.20.3)\nRequirement already satisfied: Pillow in /opt/conda/lib/python3.10/site-packages (from sentence_transformers) (9.5.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\nRequirement already satisfied: packaging>=19.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (21.3)\nCollecting pyproject_hooks (from build>=1.0.3->chromadb)\n  Downloading pyproject_hooks-1.0.0-py3-none-any.whl.metadata (1.3 kB)\nRequirement already satisfied: tomli>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (3.20.2)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain) (0.9.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.32.0.post1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.15.1->sentence_transformers) (2023.12.2)\nRequirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain) (2.4)\nRequirement already satisfied: certifi>=14.05.14 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2023.11.17)\nRequirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\nRequirement already satisfied: python-dateutil>=2.5.3 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\nRequirement already satisfied: google-auth>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.26.1)\nRequirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.7.0)\nRequirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.3.1)\nRequirement already satisfied: oauthlib>=3.2.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\nRequirement already satisfied: urllib3>=1.24.2 in /opt/conda/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.26.18)\nRequirement already satisfied: anyio<5,>=3 in /opt/conda/lib/python3.10/site-packages (from langchain-core<0.2,>=0.1.26->langchain) (4.2.0)\nCollecting packaging>=19.0 (from build>=1.0.3->chromadb)\n  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\nCollecting coloredlogs (from onnxruntime>=1.14.1->chromadb)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (23.5.26)\nRequirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (3.20.3)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.12)\nRequirement already satisfied: deprecated>=1.2.6 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.14)\nRequirement already satisfied: importlib-metadata<7.0,>=6.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (6.11.0)\nRequirement already satisfied: backoff<3.0.0,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (2.2.1)\nRequirement already satisfied: googleapis-common-protos~=1.52 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.62.0)\nRequirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nRequirement already satisfied: opentelemetry-proto==1.22.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.22.0)\nCollecting opentelemetry-instrumentation-asgi==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.44b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.44b0-py3-none-any.whl.metadata (6.1 kB)\nCollecting opentelemetry-semantic-conventions==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_semantic_conventions-0.44b0-py3-none-any.whl.metadata (2.2 kB)\nCollecting opentelemetry-util-http==0.44b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.44b0-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: setuptools>=16.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (69.0.3)\nRequirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.14.1)\nCollecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.44b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading asgiref-3.7.2-py3-none-any.whl.metadata (9.2 kB)\nINFO: pip is looking at multiple versions of opentelemetry-sdk to determine which version is compatible with other requirements. This could take a while.\nCollecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb)\n  Downloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl.metadata (2.3 kB)\nCollecting opentelemetry-instrumentation-asgi==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl.metadata (2.1 kB)\nCollecting opentelemetry-instrumentation==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.43b0 in /opt/conda/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.43b0)\nCollecting opentelemetry-util-http==0.43b0 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb)\n  Downloading opentelemetry_util_http-0.43b0-py3-none-any.whl.metadata (2.5 kB)\nCollecting monotonic>=1.5 (from posthog>=2.4.0->chromadb)\n  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.14.6)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.28->chromadb) (3.6)\nRequirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (2023.12.25)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.32.0->sentence_transformers) (0.4.2)\nRequirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.7)\nRequirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb) (0.14.0)\nRequirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.1)\nRequirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.0)\nRequirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.19.0)\nRequirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\nRequirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (12.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.2.0)\nRequirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.3.0)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain) (1.2.0)\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.17.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain) (1.0.0)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\nRequirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.5.1)\nDownloading chromadb-0.4.23-py3-none-any.whl (521 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m521.7/521.7 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading chroma_hnswlib-0.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.1.9-py3-none-any.whl (816 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading build-1.0.3-py3-none-any.whl (18 kB)\nDownloading grpcio-1.62.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading kubernetes-29.0.0-py2.py3-none-any.whl (1.6 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m63.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langsmith-0.1.7-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.1/62.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading onnxruntime-1.17.0-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading opentelemetry_instrumentation_fastapi-0.43b0-py3-none-any.whl (11 kB)\nDownloading opentelemetry_instrumentation_asgi-0.43b0-py3-none-any.whl (14 kB)\nDownloading opentelemetry_instrumentation-0.43b0-py3-none-any.whl (28 kB)\nDownloading opentelemetry_util_http-0.43b0-py3-none-any.whl (6.9 kB)\nDownloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading posthog-3.4.2-py2.py3-none-any.whl (41 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m41.2/41.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pulsar_client-3.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m98.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading packaging-23.2-py3-none-any.whl (53 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyproject_hooks-1.0.0-py3-none-any.whl (9.3 kB)\nDownloading asgiref-3.7.2-py3-none-any.whl (24 kB)\nBuilding wheels for collected packages: pypika\n  Building wheel for pypika (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pypika: filename=PyPika-0.48.9-py2.py3-none-any.whl size=53723 sha256=d0a6e970626b08dcb865d3a6ba72731dd1dab575e936873a75bd7eeb71abdaf6\n  Stored in directory: /root/.cache/pip/wheels/e1/26/51/d0bffb3d2fd82256676d7ad3003faea3bd6dddc9577af665f4\nSuccessfully built pypika\nInstalling collected packages: pypika, monotonic, pyproject_hooks, pulsar-client, packaging, orjson, opentelemetry-util-http, humanfriendly, grpcio, chroma-hnswlib, bcrypt, asgiref, posthog, coloredlogs, build, opentelemetry-instrumentation, onnxruntime, langsmith, kubernetes, opentelemetry-instrumentation-asgi, langchain-core, sentence_transformers, opentelemetry-instrumentation-fastapi, langchain-community, langchain, chromadb\n  Attempting uninstall: packaging\n    Found existing installation: packaging 21.3\n    Uninstalling packaging-21.3:\n      Successfully uninstalled packaging-21.3\n  Attempting uninstall: orjson\n    Found existing installation: orjson 3.9.10\n    Uninstalling orjson-3.9.10:\n      Successfully uninstalled orjson-3.9.10\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.51.1\n    Uninstalling grpcio-1.51.1:\n      Successfully uninstalled grpcio-1.51.1\n  Attempting uninstall: kubernetes\n    Found existing installation: kubernetes 26.1.0\n    Uninstalling kubernetes-26.1.0:\n      Successfully uninstalled kubernetes-26.1.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cubinlinker, which is not installed.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncudf 23.8.0 requires ptxcompiler, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\ngoogle-cloud-bigquery 2.34.4 requires packaging<22.0dev,>=14.3, but you have packaging 23.2 which is incompatible.\njupyterlab 4.0.11 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\njupyterlab-lsp 5.0.2 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires kubernetes<27,>=8.0.0, but you have kubernetes 29.0.0 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.7.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nosmnx 1.8.1 requires shapely>=2.0, but you have shapely 1.8.5.post1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\nspopt 0.6.0 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed asgiref-3.7.2 bcrypt-4.1.2 build-1.0.3 chroma-hnswlib-0.7.3 chromadb-0.4.23 coloredlogs-15.0.1 grpcio-1.60.0 humanfriendly-10.0 kubernetes-29.0.0 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.26 langsmith-0.1.7 monotonic-1.6 onnxruntime-1.17.0 opentelemetry-instrumentation-0.43b0 opentelemetry-instrumentation-asgi-0.43b0 opentelemetry-instrumentation-fastapi-0.43b0 opentelemetry-util-http-0.43b0 orjson-3.9.15 packaging-23.2 posthog-3.4.2 pulsar-client-3.4.0 pypika-0.48.9 pyproject_hooks-1.0.0 sentence_transformers-2.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"# install langchain experimental features (this will likely be moved to stable in the future).\n!pip install --quiet langchain_experimental","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:26:48.621868Z","iopub.execute_input":"2024-02-24T14:26:48.622814Z","iopub.status.idle":"2024-02-24T14:27:01.241134Z","shell.execute_reply.started":"2024-02-24T14:26:48.622779Z","shell.execute_reply":"2024-02-24T14:27:01.239736Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from langchain_community.document_loaders import PyPDFLoader\nfrom langchain.docstore.document import Document\n\n# Load the pdf file... By default will split into pages\nloader = PyPDFLoader(\"How to Build a Career in AI.pdf\")\npages = loader.load_and_split()","metadata":{"execution":{"iopub.status.busy":"2024-02-24T14:27:16.043297Z","iopub.execute_input":"2024-02-24T14:27:16.044065Z","iopub.status.idle":"2024-02-24T14:27:17.366999Z","shell.execute_reply.started":"2024-02-24T14:27:16.044029Z","shell.execute_reply":"2024-02-24T14:27:17.365723Z"},"trusted":true},"execution_count":5,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocstore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Load the pdf file... By default will split into pages\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m loader \u001b[38;5;241m=\u001b[39m \u001b[43mPyPDFLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow to Build a Career in AI.pdf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m pages \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload_and_split()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:157\u001b[0m, in \u001b[0;36mPyPDFLoader.__init__\u001b[0;34m(self, file_path, password, headers, extract_images)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m    155\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpypdf package not found, please install it with \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install pypdf`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    156\u001b[0m     )\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparser \u001b[38;5;241m=\u001b[39m PyPDFParser(password\u001b[38;5;241m=\u001b[39mpassword, extract_images\u001b[38;5;241m=\u001b[39mextract_images)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/langchain_community/document_loaders/pdf.py:100\u001b[0m, in \u001b[0;36mBasePDFLoader.__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(temp_pdf)\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path):\n\u001b[0;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile path \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a valid file or url\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfile_path)\n","\u001b[0;31mValueError\u001b[0m: File path How to Build a Career in AI.pdf is not a valid file or url"],"ename":"ValueError","evalue":"File path How to Build a Career in AI.pdf is not a valid file or url","output_type":"error"}]},{"cell_type":"code","source":"from langchain.embeddings import HuggingFaceBgeEmbeddings\n\n# Load an embedding model from hugging face.\nmodel_name = \"BAAI/bge-large-en-v1.5\"\nmodel_kwargs = {'device': 'cuda'}\nencode_kwargs = {'normalize_embeddings': True}\n\nembed_model = HuggingFaceBgeEmbeddings(\n    model_name=model_name,\n    model_kwargs=model_kwargs,\n    encode_kwargs=encode_kwargs\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:14:02.502954Z","iopub.execute_input":"2024-02-16T01:14:02.503662Z","iopub.status.idle":"2024-02-16T01:14:20.072514Z","shell.execute_reply.started":"2024-02-16T01:14:02.503627Z","shell.execute_reply":"2024-02-16T01:14:20.071661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_experimental.text_splitter import SemanticChunker\n\n# create a Semantic text splitter for the document, At a high level, \n# this splits into sentences, then groups into groups of 3 sentences, \n# and then merges ones that are similar in the embedding space.\ntext_splitter = SemanticChunker(embed_model)\n\n# split the pages using Semantic Chunker.\ndocuments = text_splitter.split_documents(pages)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_community.vectorstores import Chroma\n\n# embed and insert all chunks of the documents into the vector database\nvector_db = Chroma.from_documents(\n    documents, \n    embed_model, # model to use for embedding the document chunks before storing.\n    persist_directory='vector_db', # persist the database in memory.\n    collection_name='ai_career' # name of the collection to store the chunks in.\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:15:38.832354Z","iopub.execute_input":"2024-02-16T01:15:38.832711Z","iopub.status.idle":"2024-02-16T01:15:42.705172Z","shell.execute_reply.started":"2024-02-16T01:15:38.83268Z","shell.execute_reply":"2024-02-16T01:15:42.704368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# perform a vector similarity search on a query.\nquery = \"how do i start a career in ai?\"\n\n# return the chunks of the most similar five embeddings in the db\ndocs = vector_db.similarity_search(query, k=5)\n\nprint(docs[4].page_content)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:15:49.246632Z","iopub.execute_input":"2024-02-16T01:15:49.247473Z","iopub.status.idle":"2024-02-16T01:15:49.327884Z","shell.execute_reply.started":"2024-02-16T01:15:49.247432Z","shell.execute_reply":"2024-02-16T01:15:49.326824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Argumentation** ‚ûï\n\nNow that we have setup a vector database and can retrieve similar chunks to our query, we are going to combine this chunks together to form a context. This context is then passed together with our query as the prompt to our LLM.","metadata":{}},{"cell_type":"code","source":"# util function to join all retrieved documents chunks together to form a context.\ndef join_retrieved_docs(docs):\n    return \"\\n\\n\".join([doc.page_content for doc in docs])","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:16:19.258198Z","iopub.execute_input":"2024-02-16T01:16:19.258591Z","iopub.status.idle":"2024-02-16T01:16:19.263506Z","shell.execute_reply.started":"2024-02-16T01:16:19.25856Z","shell.execute_reply":"2024-02-16T01:16:19.262397Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from langchain_core.prompts import ChatPromptTemplate\n\n# Template so we can attach our context and query as prompt to the LLM on the fly.\ntemplate = \"\"\"Answer the question using vital information from the following context, \nif the context is relevant, if context given is not relevant, \nyou should reply >>> 'Sorry, But the context provided doesn't contain enough or relevant\ninformation to answer your question'\n\n>>> 'Context : {context}'\n\n>>> 'Question: {question}'\n\"\"\"\n\nprompt = ChatPromptTemplate.from_template(template)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:21:41.145318Z","iopub.execute_input":"2024-02-16T01:21:41.146201Z","iopub.status.idle":"2024-02-16T01:21:41.151148Z","shell.execute_reply.started":"2024-02-16T01:21:41.146165Z","shell.execute_reply":"2024-02-16T01:21:41.150279Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Generation** ‚úçÔ∏è","metadata":{}},{"cell_type":"code","source":"from langchain.llms import HuggingFaceHub\nimport os\n\nos.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"<Your Hugging Face API Token Here>\"\n# Note: To get an API Token, sign up to hugging face -> go to settings -> access token\n# -> create new token -> copy the token and paste above üëÜ.","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:21:43.903142Z","iopub.execute_input":"2024-02-16T01:21:43.90406Z","iopub.status.idle":"2024-02-16T01:21:43.909088Z","shell.execute_reply.started":"2024-02-16T01:21:43.904018Z","shell.execute_reply":"2024-02-16T01:21:43.908041Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"repo_id = \"google/flan-t5-xxl\" # we will be using google flan LLM from hugging face.\n\nllm = HuggingFaceHub(\n    repo_id=repo_id,\n    # params for LLM text generation\n    model_kwargs={\n        \"temperature\": 0.1 # low values means more precise generation.\n    }\n)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:21:46.296521Z","iopub.execute_input":"2024-02-16T01:21:46.296884Z","iopub.status.idle":"2024-02-16T01:21:46.628371Z","shell.execute_reply.started":"2024-02-16T01:21:46.296857Z","shell.execute_reply":"2024-02-16T01:21:46.627159Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the LLMChain class from langchain.chains module\nfrom langchain.chains import LLMChain\n\n# Define a function that takes a question, a language model, and a prompt template as arguments\ndef query_llm_with_context(question, llm, prompt_template):\n    \n    # Use the vector database to find the most similar document chunks to the question\n    # The parameter k specifies the number of document chunks to retrieve\n    context = vector_db.similarity_search(question, k=5)\n\n    # Create an instance of the LLMChain class\n    # The prompt parameter specifies the format of the input for the language model\n    # The llm parameter specifies the name of the language model to use\n    llm_chain = LLMChain(\n        prompt=prompt_template, \n        llm=llm\n    )\n\n    # Invoke the LLMChain instance with the input dictionary\n    # The input dictionary contains the context and the question keys\n    # The context key contains the concatenated document chunks retrieved from the vector database\n    # The question key contains the original question to the LLM\n    llm_response = llm_chain.invoke(\n        input = {\n            'context' : join_retrieved_docs(context),\n            'question' : question\n                })\n    \n    # Return the text of the response generated by the language model\n    return llm_response['text']\n","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:21:49.432681Z","iopub.execute_input":"2024-02-16T01:21:49.433054Z","iopub.status.idle":"2024-02-16T01:21:49.439682Z","shell.execute_reply.started":"2024-02-16T01:21:49.433025Z","shell.execute_reply":"2024-02-16T01:21:49.438591Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Testing**","metadata":{}},{"cell_type":"code","source":"query = 'what is recommended for new ai startups to do?'\n\nquery_llm_with_context(question=query, llm=llm, prompt_template=prompt)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:21:52.937877Z","iopub.execute_input":"2024-02-16T01:21:52.93859Z","iopub.status.idle":"2024-02-16T01:21:56.795055Z","shell.execute_reply.started":"2024-02-16T01:21:52.938556Z","shell.execute_reply":"2024-02-16T01:21:56.794092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"query2 = 'What is Russian Roulette'\n\nquery_llm_with_context(question=query2, llm=llm, prompt_template=prompt)","metadata":{"execution":{"iopub.status.busy":"2024-02-16T01:22:00.0385Z","iopub.execute_input":"2024-02-16T01:22:00.038862Z","iopub.status.idle":"2024-02-16T01:22:01.181826Z","shell.execute_reply.started":"2024-02-16T01:22:00.038834Z","shell.execute_reply":"2024-02-16T01:22:01.180845Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"üëÜüëÜ As you can see in the first example the LLM used our context as the source to generate an answer for us. While in the second example, instead of hallucinating the LLM simply replied that there isn't enough or revelant context to answer us.","metadata":{}},{"cell_type":"markdown","source":"### **Final Words**\nThis tutorial simply introduced you to a RAG techniques & implementation, in production more complex RAG techinques like Sentence-Window retrival, Auto-merging retrival e.t.c are used to improve context relevance. We also use tools üèπ like TruERA for LLM response Evaluation.\n\n**Congratulations** üéâüéâ\nYou can now use fundermental RAG techniques. üòä\n\nFollow me on:\n\n* **[LinkedIn Profile](https://www.linkedin.com/in/jonathan-okorie-843126216/)** for questions, deep learning projects, chat e.t.c.\n\n* **[Twitter Profile](https://twitter.com/Nathan_Young_1)** for bite-sized knowledge & (questionable) puns. \n\n* **[Kaggle Profile](https://www.kaggle.com/nathanyoung1)** to be notified when i create a new detailed notebook explanation. ","metadata":{}}]}